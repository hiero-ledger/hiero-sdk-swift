name: Swift CI
on:
  pull_request:
  push: 
    branches: ['main']

defaults:
  run:
    shell: bash

permissions:
  contents: read

jobs:
  format:
    runs-on: macos-latest
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@0080882f6c36860b6ba35c610c98ce87d4e2f26f # v2.10.2
        with:
          egress-policy: audit

      - name: Checkout Code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Install swift-format
        run: brew install swift-format

      - name: Format
        run: swift format lint --strict --configuration .swift-format.json --recursive --parallel Sources/ Tests/ Examples/ Package.swift

  build:
    strategy:
      matrix:
        swift: ["5.9"]
        os: [macos-13]

    runs-on: ${{ matrix.os }}
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@0080882f6c36860b6ba35c610c98ce87d4e2f26f # v2.10.2
        with:
          egress-policy: audit

      - name: Setup Swift
        uses: swift-actions/setup-swift@3aed395c5397f62deb91d8fe7af1418a9ae4d16f # v2.1.0
        with:
          swift-version: ${{ matrix.swift }}

      - name: Checkout Code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Cache
        uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
        with:
          path: sdk/swift/.build
          key: ${{ runner.os }}-${{ matrix.swift }}-spm-${{ github.job }}-${{ hashFiles('**/Package.resolved') }}
          restore-keys: |
            ${{ runner.os }}-${{ matrix.swift }}-spm-

      - name: Build
        run: swift build

  test:
    strategy:
      matrix:
        swift: ["5.9"]
        os: [macos-13]

    needs: [build]
    runs-on: ${{ matrix.os }}

    env:
      SOLO_CLUSTER_NAME: solo
      SOLO_NAMESPACE: solo
      SOLO_CLUSTER_SETUP_NAMESPACE: solo-cluster
      SOLO_DEPLOYMENT: solo-deployment
      KIND_IMAGE: kindest/node:v1.34.0

    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@0080882f6c36860b6ba35c610c98ce87d4e2f26f # v2.10.2
        with:
          egress-policy: audit

      - name: Setup Swift
        uses: swift-actions/setup-swift@3aed395c5397f62deb91d8fe7af1418a9ae4d16f # v2.1.0
        with:
          swift-version: ${{ matrix.swift }}

      - name: Checkout Code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Cache Code
        uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
        with:
          path: sdk/swift/.build
          key: ${{ runner.os }}-${{ matrix.swift }}-spm-${{ github.job }}-${{ hashFiles('**/Package.resolved') }}
          restore-keys: |
            ${{ runner.os }}-${{ matrix.swift }}-spm-

      # ---------- Tooling ----------
      - name: Install kind / kubectl / Helm / utils
        run: |
          brew update
          brew install kind kubectl helm jq coreutils

      - name: Remove Preinstalled Docker
        run: |
          set -x
          sudo killall dockerd || true
          sudo killall containerd || true
          sudo rm -rvf /usr/bin/*containerd* || true
          sudo rm -rvf /usr/bin/docker* || true
          sudo rm -rvf /usr/local/bin/docker* || true
          sudo rm -rvf /usr/local/bin/*lima* || true

      - name: Start CoLima
        run: |
          brew install docker
          brew install colima
          colima start --vm-type=vz --cpu=2 --memory=4 --disk=10

      - name: Show CoLima Logs
        if: failure()
        run: |
          echo "::group::Serial Logs"
          cat ${HOME}/.colima/_lima/colima/serial*.log
          echo "::endgroup::"
          
          echo "::group::Error Logs"
          cat ${HOME}/.colima/_lima/colima/ha.stderr.log
          echo "::endgroup::"

      # ---------- Node.js for Solo ----------
      - name: Use Node 20.18.x
        uses: actions/setup-node@v4
        with:
          node-version: "20.18.x"
      - name: Verify Node
        run: |
          node -v
          npm -v

      # ---------- Solo CLI ----------
      - name: Install Solo CLI (v0.38.x)
        run: |
          npm install -g @hashgraph/solo@0.38
          solo --version

      # ---------- Create a local K8s cluster with kind ----------
      - name: Create kind cluster
        shell: bash
        run: |
          kind create cluster -n "${SOLO_CLUSTER_NAME}" --image "${KIND_IMAGE}"
          kubectl cluster-info --context "kind-${SOLO_CLUSTER_NAME}"

      - name: Debug kube-apiserver (on failure)
        if: failure()
        run: |
          set -x
          NODE="${SOLO_CLUSTER_NAME}-control-plane"
          docker ps -a || true
          # Show node logs
          docker logs "${NODE}" | sed -n '1,200p' || true
          # Inspect apiserver container via crictl inside the node
          docker exec -i "${NODE}" crictl --runtime-endpoint /run/containerd/containerd.sock ps -a | grep kube-apiserver || true
          APISERVER_ID="$(docker exec -i "${NODE}" crictl --runtime-endpoint /run/containerd/containerd.sock ps -a | awk '/kube-apiserver/ {print $1; exit}')"
          if [ -n "${APISERVER_ID}" ]; then
            docker exec -i "${NODE}" crictl --runtime-endpoint /run/containerd/containerd.sock logs "${APISERVER_ID}" || true
          fi

      - name: Export kind logs (on failure)
        if: failure()
        run: kind export logs --name "${SOLO_CLUSTER_NAME}" ./kind-logs || true

      - name: Upload kind logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: kind-logs
          path: ./kind-logs

      # ---------- Solo initialization & deployment (minimal path) ----------
      - name: Solo init
        shell: bash
        run: |
          # Clean artifacts if re-running the job (optional safety)
          for cluster in $(kind get clusters); do kind delete cluster -n "$cluster"; done || true
          rm -rf ~/.solo || true
          # Recreate the cluster after cleanup (when above deletion occurred)
          kind get clusters | grep -qx "${SOLO_CLUSTER_NAME}" || kind create cluster -n "${SOLO_CLUSTER_NAME}" --image "${KIND_IMAGE}"
          # Initialize solo (checks dependencies incl. helm)
          solo init

      - name: Connect cluster-ref and create deployment
        shell: bash
        run: |
          solo cluster-ref connect --cluster-ref "kind-${SOLO_CLUSTER_NAME}" --context "kind-${SOLO_CLUSTER_NAME}"
          solo deployment create -n "${SOLO_NAMESPACE}" --deployment "${SOLO_DEPLOYMENT}"

      - name: Add a cluster (1 consensus node)
        shell: bash
        run: |
          solo deployment add-cluster \
            --deployment "${SOLO_DEPLOYMENT}" \
            --cluster-ref "kind-${SOLO_CLUSTER_NAME}" \
            --num-consensus-nodes 1

      - name: Generate node keys (gossip + TLS)
        shell: bash
        run: |
          solo node keys --gossip-keys --tls-keys --deployment "${SOLO_DEPLOYMENT}"
          ls -la ~/.solo/cache/keys || true

      - name: Setup cluster shared components
        shell: bash
        run: |
          solo cluster-ref setup -s "${SOLO_CLUSTER_SETUP_NAMESPACE}"

      - name: Deploy network (consensus node only)
        shell: bash
        run: |
          # This charts install may take a bitâ€”prints will help you see progress
          set -x
          solo network deploy --deployment "${SOLO_DEPLOYMENT}" --cluster-ref "kind-${SOLO_CLUSTER_NAME}"

      - name: Set up Hedera node
        shell: bash
        run: |
          solo node setup --deployment "${SOLO_DEPLOYMENT}" --cluster-ref "kind-${SOLO_CLUSTER_NAME}"

      - name: Start Hedera node(s)
        shell: bash
        run: |
          solo node start --deployment "${SOLO_DEPLOYMENT}" --cluster-ref "kind-${SOLO_CLUSTER_NAME}"

      - name: Wait for pods to become Ready
        shell: bash
        run: |
          # Wait up to ~10 minutes for all pods in the Solo namespace to be ready
          kubectl -n "${SOLO_NAMESPACE}" get pods
          end=$((SECONDS+600))
          while [ $SECONDS -lt $end ]; do
            not_ready=$(kubectl -n "${SOLO_NAMESPACE}" get pods --no-headers | awk '{print $2}' | grep -v '^1/1$' || true)
            if [ -z "$not_ready" ]; then
              echo "All pods ready."
              kubectl -n "${SOLO_NAMESPACE}" get pods -o wide
              exit 0
            fi
            echo "Pods not ready yet; sleeping 10s..."
            sleep 10
          done
          echo "Timed out waiting for pods."
          kubectl -n "${SOLO_NAMESPACE}" get pods -o wide
          exit 1

      # ---------- (Optional) Show endpoints / basic sanity ----------
      - name: Show services and endpoints
        if: always()
        shell: bash
        run: |
          echo "Solo services in ${SOLO_NAMESPACE}:"
          kubectl -n "${SOLO_NAMESPACE}" get svc
          echo "Solo endpoints (if any):"
          kubectl -n "${SOLO_NAMESPACE}" get endpoints

      # ---------- Cleanup on failure to avoid runner leftovers ----------
      - name: Cleanup kind cluster (on failure)
        if: failure()
        run: |
          kind delete cluster -n "${SOLO_CLUSTER_NAME}" || true
          colima stop || true
          # Dump colima logs
          LOGDIR="$HOME/.colima/_lima/colima"
          echo "=== ha.stderr.log ===" ; cat "$LOGDIR/ha.stderr.log" || true
          echo "=== serial logs ==="   ; cat "$LOGDIR"/serial*.log   || true

      - name: Write .env for tests
        shell: bash
        run: |
          umask 077
          {
            echo "TEST_NETWORK_NAME=localhost"
            echo "TEST_RUN_NONFREE=1"
            echo "TEST_OPERATOR_ID=0.0.2"
            echo "TEST_OPERATOR_KEY=302e020100300506032b65700422042091132178e72057a1d7528025956fe39b0b847f200ab59b2fdd367017f3087137"
          } > .env
          cat .env

      - name: Test
        run: swift test
