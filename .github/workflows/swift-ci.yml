name: Swift CI
on:
  pull_request:
  push: 
    branches: ['main']

defaults:
  run:
    shell: bash

permissions:
  contents: read

jobs:
  format:
    runs-on: macos-latest
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@0080882f6c36860b6ba35c610c98ce87d4e2f26f # v2.10.2
        with:
          egress-policy: audit

      - name: Checkout Code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Install swift-format
        run: brew install swift-format

      - name: Format
        run: swift format lint --strict --configuration .swift-format.json --recursive --parallel Sources/ Tests/ Examples/ Package.swift

  # build:
  #   strategy:
  #     matrix:
  #       swift: ["5.9"]
  #       os: [macos-13]

  #   runs-on: ${{ matrix.os }}
  #   steps:
  #     - name: Harden Runner
  #       uses: step-security/harden-runner@0080882f6c36860b6ba35c610c98ce87d4e2f26f # v2.10.2
  #       with:
  #         egress-policy: audit

  #     - name: Setup Swift
  #       uses: swift-actions/setup-swift@3aed395c5397f62deb91d8fe7af1418a9ae4d16f # v2.1.0
  #       with:
  #         swift-version: ${{ matrix.swift }}

  #     - name: Checkout Code
  #       uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

  #     - name: Cache
  #       uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
  #       with:
  #         path: sdk/swift/.build
  #         key: ${{ runner.os }}-${{ matrix.swift }}-spm-${{ github.job }}-${{ hashFiles('**/Package.resolved') }}
  #         restore-keys: |
  #           ${{ runner.os }}-${{ matrix.swift }}-spm-
  #     - name: Build
  #       run: swift build

  test:
    strategy:
      matrix:
        swift: ["5.9"]
        os: [macos-13]
    runs-on: ${{ matrix.os }}
    env:
      SOLO_CLUSTER_NAME: solo
      SOLO_NAMESPACE: solo
      SOLO_CLUSTER_SETUP_NAMESPACE: solo-cluster
      SOLO_DEPLOYMENT: solo-deployment
      KIND_IMAGE: kindest/node:v1.32.2   # pin to a stable image

    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@0080882f6c36860b6ba35c610c98ce87d4e2f26f
        with:
          egress-policy: audit

      - name: Setup Swift
        uses: swift-actions/setup-swift@3aed395c5397f62deb91d8fe7af1418a9ae4d16f
        with:
          swift-version: ${{ matrix.swift }}

      - name: Checkout Code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683

      - name: Cache Code
        uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57
        with:
          path: sdk/swift/.build
          key: ${{ runner.os }}-${{ matrix.swift }}-spm-${{ github.job }}-${{ hashFiles('**/Package.resolved') }}
          restore-keys: |
            ${{ runner.os }}-${{ matrix.swift }}-spm-

      # ---------- Tooling ----------
      - name: Install kind / kubectl / Helm / utils
        run: |
          set -euxo pipefail
          brew update
          brew install kind kubectl helm jq coreutils

      - name: Remove Preinstalled Docker (clean path)
        run: |
          set -x
          sudo killall dockerd || true
          sudo killall containerd || true
          sudo rm -rvf /usr/bin/*containerd* || true
          sudo rm -rvf /usr/bin/docker* || true
          sudo rm -rvf /usr/local/bin/docker* || true
          sudo rm -rvf /usr/local/bin/*lima* || true

      - name: Start Colima (Docker runtime, more resources)
        run: |
          set -euxo pipefail
          brew install docker colima
          colima start --vm-type=vz --runtime docker --cpu=4 --memory=8 --disk=20
          docker context use colima
          docker version
          docker info | sed -n '1,120p'

      - name: Show Colima Logs (on failure)
        if: failure()
        run: |
          echo "::group::Serial Logs"
          cat ${HOME}/.colima/_lima/colima/serial*.log || true
          echo "::endgroup::"
          echo "::group::Error Logs"
          cat ${HOME}/.colima/_lima/colima/ha.stderr.log || true
          echo "::endgroup::"

      # ---------- Node.js for Solo ----------
      - name: Use Node 20.18.x
        uses: actions/setup-node@v4
        with:
          node-version: "20.18.x"

      - name: Install Solo CLI (v0.47.x)
        run: |
          npm install -g @hashgraph/solo@0.47
          solo --version

      # ---------- Create a local K8s cluster with KinD (robust) ----------
      - name: Pre-pull KinD node image
        run: docker pull "${KIND_IMAGE}"

      - name: Create KinD cluster (retry + wait)
        run: |
          set -euxo pipefail
          retry() { n=0; until [ $n -ge 3 ]; do "$@" && break; n=$((n+1)); echo "retry $n..."; sleep $((5*n)); done; [ $n -lt 3 ]; }
          # Clean any leftovers
          kind get clusters | xargs -r -I{} kind delete cluster --name {}
          retry kind create cluster --name "${SOLO_CLUSTER_NAME}" --image "${KIND_IMAGE}" --wait 5m
          kubectl cluster-info --context "kind-${SOLO_CLUSTER_NAME}"
          kubectl get nodes -o wide

      - name: Debug kube-apiserver (on failure)
        if: failure()
        run: |
          set -x
          NODE="${SOLO_CLUSTER_NAME}-control-plane"
          docker ps -a || true
          docker logs "${NODE}" | sed -n '1,200p' || true
          docker exec -i "${NODE}" crictl --runtime-endpoint /run/containerd/containerd.sock ps -a | grep kube-apiserver || true
          APISERVER_ID="$(docker exec -i "${NODE}" crictl --runtime-endpoint /run/containerd/containerd.sock ps -a | awk '/kube-apiserver/ {print $1; exit}')"
          if [ -n "${APISERVER_ID}" ]; then
            docker exec -i "${NODE}" crictl --runtime-endpoint /run/containerd/containerd.sock logs "${APISERVER_ID}" || true
          fi
          kind export logs --name "${SOLO_CLUSTER_NAME}" ./kind-logs || true

      - name: Upload kind logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: kind-logs
          path: ./kind-logs

      # ---------- Solo initialization & deployment ----------
      - name: Initialize Solo
        run: |
          set -euxo pipefail
          # Do NOT delete/recreate the cluster here anymore
          rm -rf ~/.solo || true
          solo init

      - name: Connect the Solo CLI to the kind cluster using a cluster reference name
        run: solo cluster-ref config connect --cluster-ref "kind-${SOLO_CLUSTER_NAME}" --context "kind-${SOLO_CLUSTER_NAME}"

      - name: Create deployment
        run: solo deployment config create -n "${SOLO_NAMESPACE}" --deployment "${SOLO_DEPLOYMENT}"

      - name: Add the kind cluster to the deployment with one consensus node
        run: solo deployment cluster attach --deployment "${SOLO_DEPLOYMENT}" --cluster-ref kind-${SOLO_CLUSTER_NAME} --num-consensus-nodes 1

      - name: Generate node keys
        run: solo keys consensus generate --gossip-keys --tls-keys -i node1 --deployment "${SOLO_DEPLOYMENT}"

      - name: Setup the Solo cluster
        run: solo cluster-ref config setup -s "${SOLO_CLUSTER_SETUP_NAMESPACE}"

      - name: Deploy network
        run: solo consensus network deploy -i node1 --deployment "${SOLO_DEPLOYMENT}"

      - name: Setup node
        run: solo consensus node setup -i node1 --deployment "${SOLO_DEPLOYMENT}"

      - name: Start node
        run: solo consensus node start -i node1 --deployment "${SOLO_DEPLOYMENT}"

      - name: Deploy mirror node
        run: solo mirror node add --cluster-ref kind-${SOLO_CLUSTER_NAME} --deployment "${SOLO_DEPLOYMENT}" --mirror-node-version v0.138.0 --pinger --dev

      - name: Wait for workloads to be Ready (robust)
        run: |
          set -euxo pipefail

          app_ns="${SOLO_NAMESPACE}"
          sys_ns="${SOLO_CLUSTER_SETUP_NAMESPACE}"

          # Sanity: which context are we on?
          kubectl config current-context
          kubectl get nodes -o wide

          # 0) Quick view immediately after Solo steps (helps when flaking)
          echo "::group::Helm releases"
          helm ls -A || true
          echo "::endgroup::"

          echo "::group::Namespaces"
          kubectl get ns
          echo "::endgroup::"

          # 1) Ensure both namespaces exist (sys first, then app)
          for ns in "$sys_ns" "$app_ns"; do
            end=$((SECONDS+300))
            until kubectl get ns "$ns" >/dev/null 2>&1; do
              [ $SECONDS -ge $end ] && { echo "timeout: namespace $ns not found"; kubectl get ns; exit 1; }
              sleep 3
            done
          done

          # 2) Shared components should come up first (cert-manager, etc.)
          # If there are controllers here, wait for them; don't fail if there aren't any.
          kubectl -n "$sys_ns" get deploy -o name | xargs -r -L1 kubectl -n "$sys_ns" rollout status --timeout=10m || true
          kubectl -n "$sys_ns" get sts   -o name | xargs -r -L1 kubectl -n "$sys_ns" rollout status --timeout=10m || true

          # 3) Wait until some workload shows in app namespace (not just the namespace itself)
          end=$((SECONDS+600))
          until [ "$(kubectl -n "$app_ns" get pods --no-headers 2>/dev/null | wc -l | xargs || echo 0)" -gt 0 ]; do
            [ $SECONDS -ge $end ] && {
              echo "timeout: no pods appeared in $app_ns"
              echo "::group::All resources across namespaces"
              kubectl get all -A
              echo "::endgroup::"
              echo "::group::Recent events"
              kubectl get events -A --sort-by=.lastTimestamp | tail -n 200 || true
              echo "::endgroup::"
              exit 1
            }
            sleep 5
          done

          # 4) Rollout controllers in the app namespace (if present), then wait for pods
          kubectl -n "$app_ns" get deploy -o name | xargs -r -L1 kubectl -n "$app_ns" rollout status --timeout=10m || true
          kubectl -n "$app_ns" get sts   -o name | xargs -r -L1 kubectl -n "$app_ns" rollout status --timeout=10m || true

          # Exclude one-shot jobs from readiness wait
          pods_exist=$(kubectl -n "$app_ns" get pod -o name | wc -l | xargs || echo 0)
          if [ "$pods_exist" -gt 0 ]; then
            kubectl -n "$app_ns" wait pod -l '!job-name' --for=condition=Ready --timeout=10m || true
          fi

          kubectl -n "$app_ns" get all -o wide

      - name: Show services and endpoints
        run: |
          echo "Listing services in namespace ${SOLO_NAMESPACE}"
          kubectl get svc -n ${SOLO_NAMESPACE}

      - name: Port forward (if services exist)
        run: |
          if kubectl get svc haproxy-node1-svc -n ${SOLO_NAMESPACE} >/dev/null 2>&1; then
            kubectl port-forward svc/haproxy-node1-svc -n ${SOLO_NAMESPACE} 50211:50211 &
          else
            echo "HAProxy service haproxy-node1-svc not found, skipping port-forward"
          fi

          if kubectl get svc envoy-proxy-node1-svc -n ${SOLO_NAMESPACE} >/dev/null 2>&1; then
            kubectl port-forward svc/envoy-proxy-node1-svc -n ${SOLO_NAMESPACE} 8080:8080 &
          else
            echo "gRPC proxy service envoy-proxy-node1-svc not found, skipping port-forward"
          fi

          if kubectl get svc mirror-1-rest -n ${SOLO_NAMESPACE} >/dev/null 2>&1; then
            kubectl port-forward svc/mirror-1-rest -n ${SOLO_NAMESPACE} 5551:80 &
          else
            echo "Mirror node service mirror-1-rest not found, skipping port-forward"
          fi

          if kubectl get svc mirror-1-grpc -n ${SOLO_NAMESPACE} >/dev/null 2>&1; then
            kubectl port-forward svc/mirror-1-grpc -n ${SOLO_NAMESPACE} 5600:5600 &
          else
            echo "Mirror node service mirror-1-grpc not found, skipping port-forward"
          fi

          if kubectl get svc mirror-1-web3 ${SOLO_NAMESPACE} >/dev/null 2>&1; then
            kubectl port-forward svc/mirror-1-web3 -n ${SOLO_NAMESPACE} 8545:80 &
          else
            echo "Mirror node service mirror-1-web3 not found, skipping port-forward"
          fi

          if kubectl get svc mirror-1-restjava -n ${SOLO_NAMESPACE} >/dev/null 2>&1; then
            kubectl port-forward svc/mirror-1-restjava -n ${SOLO_NAMESPACE} 8084:80 &
          else
            echo "Java REST API service mirror-1-restjava not found, skipping port-forward"
          fi

      - name: Create ECDSA account
        run: |
          npm run solo -- ledger account create --generate-ecdsa-key --deployment ${SOLO_DEPLOYMENT}
          npm run solo -- ledger account update --account-id 0.0.1002 --hbar-amount 1000000 --deployment ${SOLO_DEPLOYMENT}

      - name: Write .env for tests
        run: |
          umask 077
          {
            echo "TEST_NETWORK_NAME=localhost"
            echo "TEST_RUN_NONFREE=1"
            echo "TEST_OPERATOR_ID=0.0.2"
            echo "TEST_OPERATOR_KEY=302e020100300506032b65700422042091132178e72057a1d7528025956fe39b0b847f200ab59b2fdd367017f3087137"
          } > .env
          cat .env

      - name: Test
        run: swift test

      - name: Cleanup KinD / Colima (always)
        run: |
          kind delete cluster --name "${SOLO_CLUSTER_NAME}" || true
          colima stop || true
          LOGDIR="$HOME/.colima/_lima/colima"
          echo "=== ha.stderr.log ===" ; cat "$LOGDIR/ha.stderr.log" || true
          echo "=== serial logs ==="   ; cat "$LOGDIR"/serial*.log   || true
