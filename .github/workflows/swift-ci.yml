name: Swift CI
on:
  pull_request:
  push:
    branches: ['main']

defaults:
  run:
    shell: bash

permissions:
  contents: read

jobs:
  format:
    # Run linting on Linux
    runs-on: ubuntu-22.04
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@0080882f6c36860b6ba35c610c98ce87d4e2f26f # v2.10.2
        with:
          egress-policy: audit

      - name: Checkout Code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Install system deps for Swift
        run: |
          set -euxo pipefail
          sudo apt-get update
          sudo apt-get install -y \
            git curl unzip xz-utils ca-certificates pkg-config \
            clang libicu-dev libxml2-dev libsqlite3-dev zlib1g-dev \
            libcurl4-openssl-dev libssl-dev

      - name: Setup Swift (toolchain)
        uses: swift-actions/setup-swift@3aed395c5397f62deb91d8fe7af1418a9ae4d16f # v2.1.0
        with:
          # Use same version as your test matrix unless you want to expand it
          swift-version: '5.9'

      - name: Build swift-format (Linux)
        run: |
          set -euxo pipefail
          git clone --depth=1 https://github.com/apple/swift-format.git .swift-format-src
          pushd .swift-format-src
          swift build -c release
          echo "$PWD/.build/release" >> $GITHUB_PATH
          popd
          swift-format --version || true

      - name: Format (lint)
        run: |
          swift-format lint --strict --configuration .swift-format.json --recursive --parallel Sources/ Tests/ Examples/ Package.swift

  test:
    name: Build and Test SDK on Linux
    strategy:
      matrix:
        swift: ["5.9"]
        os: [ubuntu-22.04]
    runs-on: hiero-client-sdk-linux-large
    env:
      SOLO_CLUSTER_NAME: solo
      SOLO_NAMESPACE: solo
      SOLO_CLUSTER_SETUP_NAMESPACE: solo-cluster
      SOLO_DEPLOYMENT: solo-deployment
      KIND_IMAGE: kindest/node:v1.32.2   # pin to a stable image

    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@0080882f6c36860b6ba35c610c98ce87d4e2f26f
        with:
          egress-policy: audit

      - name: Setup Swift
        uses: SwiftyLab/setup-swift@v1
        with:
          swift-version: ${{ matrix.swift }}

      - name: Checkout Code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683

      - name: Cache SPM build dir
        uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57
        with:
          path: .build
          key: ${{ runner.os }}-${{ matrix.swift }}-spm-${{ github.job }}-${{ hashFiles('**/Package.resolved') }}
          restore-keys: |
            ${{ runner.os }}-${{ matrix.swift }}-spm-

      - name: Install system dependencies for Swift/gRPC
        run: |
          set -euxo pipefail
          sudo apt-get update
          sudo apt-get install -y \
            git curl jq coreutils ca-certificates \
            clang libicu-dev libxml2-dev libsqlite3-dev zlib1g-dev \
            libcurl4-openssl-dev libssl-dev pkg-config \
            protobuf-compiler

      - name: Verify Swift installation
        run: |
          swift --version
          which swift
          swift package --version

      - name: Build SDK (release mode)
        run: |
          echo "=========================================="
          echo "Building Hiero SDK in release mode"
          echo "=========================================="
          swift build -c release --verbose

      - name: Build SDK (debug mode)
        run: |
          echo "=========================================="
          echo "Building Hiero SDK in debug mode"
          echo "=========================================="
          swift build -c debug

      - name: Build tests (compile only, no execution)
        run: |
          echo "=========================================="
          echo "Compiling test targets"
          echo "=========================================="
          swift build --build-tests

      # TODO: Uncomment when ready to run tests
      # - name: Run unit tests (HieroTests)
      #   run: swift test --filter HieroTests
      #
      # - name: Run integration tests (HieroE2ETests)
      #   run: swift test --filter HieroE2ETests

      # # ---------- Docker is preinstalled on ubuntu runners ----------
      # - name: Show Docker info
      #   run: |
      #     docker version
      #     docker info | sed -n '1,120p'

      # # ---------- Install kind / kubectl / Helm ----------
      # - name: Install kind
      #   run: |
      #     set -euxo pipefail
      #     KIND_VER="v0.23.0"
      #     curl -fsSL -o kind "https://kind.sigs.k8s.io/dl/${KIND_VER}/kind-linux-amd64"
      #     chmod +x kind
      #     sudo mv kind /usr/local/bin/kind
      #     kind --version

      # - name: Install kubectl
      #   run: |
      #     set -euxo pipefail
      #     KUBECTL_VER="$(curl -fsSL https://dl.k8s.io/release/stable.txt)"
      #     curl -fsSL -o kubectl "https://dl.k8s.io/release/${KUBECTL_VER}/bin/linux/amd64/kubectl"
      #     curl -fsSL "https://dl.k8s.io/${KUBECTL_VER}/bin/linux/amd64/kubectl.sha256" -o kubectl.sha256
      #     echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check - || true
      #     chmod +x kubectl
      #     sudo mv kubectl /usr/local/bin/kubectl
      #     kubectl version --client=true

      # - name: Install Helm
      #   run: |
      #     set -euxo pipefail
      #     curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
      #     helm version

      # # ---------- Node.js for Solo ----------
      # - name: Use Node 20.18.x
      #   uses: actions/setup-node@v4
      #   with:
      #     node-version: "20.18.x"

      # - name: Install Solo CLI (v0.47.x)
      #   run: |
      #     npm install -g @hashgraph/solo@0.47
      #     solo --version

      # # ---------- Create a local K8s cluster with KinD ----------
      # - name: Pre-pull KinD node image
      #   run: docker pull "${KIND_IMAGE}"

      # - name: Create KinD cluster (retry + wait)
      #   run: |
      #     set -euxo pipefail
      #     retry() { n=0; until [ $n -ge 3 ]; do "$@" && break; n=$((n+1)); echo "retry $n..."; sleep $((5*n)); done; [ $n -lt 3 ]; }
      #     # Clean any leftovers
      #     kind get clusters | xargs -r -I{} kind delete cluster --name {}
      #     retry kind create cluster --name "${SOLO_CLUSTER_NAME}" --image "${KIND_IMAGE}" --wait 5m
      #     kubectl cluster-info --context "kind-${SOLO_CLUSTER_NAME}"
      #     kubectl get nodes -o wide

      # - name: Debug kube-apiserver (on failure)
      #   if: failure()
      #   run: |
      #     set -x
      #     NODE="${SOLO_CLUSTER_NAME}-control-plane"
      #     docker ps -a || true
      #     docker logs "${NODE}" | sed -n '1,200p' || true
      #     docker exec -i "${NODE}" crictl --runtime-endpoint /run/containerd/containerd.sock ps -a | grep kube-apiserver || true
      #     APISERVER_ID="$(docker exec -i "${NODE}" crictl --runtime-endpoint /run/containerd/containerd.sock ps -a | awk '/kube-apiserver/ {print $1; exit}')"
      #     if [ -n "${APISERVER_ID}" ]; then
      #       docker exec -i "${NODE}" crictl --runtime-endpoint /run/containerd/containerd.sock logs "${APISERVER_ID}" || true
      #     fi
      #     kind export logs --name "${SOLO_CLUSTER_NAME}" ./kind-logs || true

      # - name: Upload kind logs (on failure)
      #   if: failure()
      #   uses: actions/upload-artifact@v4
      #   with:
      #     name: kind-logs
      #     path: ./kind-logs

      # # ---------- Solo initialization & deployment ----------
      # - name: Initialize Solo
      #   run: |
      #     set -euxo pipefail
      #     rm -rf ~/.solo || true
      #     solo init

      # - name: Connect the Solo CLI to the kind cluster using a cluster reference name
      #   run: solo cluster-ref config connect --cluster-ref "kind-${SOLO_CLUSTER_NAME}" --context "kind-${SOLO_CLUSTER_NAME}"

      # - name: Create deployment
      #   run: solo deployment config create -n "${SOLO_NAMESPACE}" --deployment "${SOLO_DEPLOYMENT}"

      # - name: Add the kind cluster to the deployment with one consensus node
      #   run: solo deployment cluster attach --deployment "${SOLO_DEPLOYMENT}" --cluster-ref kind-${SOLO_CLUSTER_NAME} --num-consensus-nodes 1

      # - name: Generate node keys
      #   run: solo keys consensus generate --gossip-keys --tls-keys -i node1 --deployment "${SOLO_DEPLOYMENT}"

      # - name: Setup the Solo cluster
      #   run: solo cluster-ref config setup -s "${SOLO_CLUSTER_SETUP_NAMESPACE}"

      # - name: Deploy network
      #   run: solo consensus network deploy -i node1 --deployment "${SOLO_DEPLOYMENT}"

      # - name: Setup node
      #   run: solo consensus node setup -i node1 --deployment "${SOLO_DEPLOYMENT}"

      # - name: Start node
      #   run: solo consensus node start -i node1 --deployment "${SOLO_DEPLOYMENT}"

      # - name: Deploy mirror node
      #   run: solo mirror node add --cluster-ref kind-${SOLO_CLUSTER_NAME} --deployment "${SOLO_DEPLOYMENT}" --mirror-node-version v0.138.0 --pinger --dev

      # - name: Wait for workloads to be Ready (robust)
      #   run: |
      #     set -euxo pipefail

      #     app_ns="${SOLO_NAMESPACE}"
      #     sys_ns="${SOLO_CLUSTER_SETUP_NAMESPACE}"

      #     kubectl config current-context
      #     kubectl get nodes -o wide

      #     echo "::group::Helm releases"
      #     helm ls -A || true
      #     echo "::endgroup::"

      #     echo "::group::Namespaces"
      #     kubectl get ns
      #     echo "::endgroup::"

      #     # 1) Ensure both namespaces exist
      #     for ns in "$sys_ns" "$app_ns"; do
      #       end=$((SECONDS+300))
      #       until kubectl get ns "$ns" >/dev/null 2>&1; do
      #         [ $SECONDS -ge $end ] && { echo "timeout: namespace $ns not found"; kubectl get ns; exit 1; }
      #         sleep 3
      #       done
      #     done

      #     # 2) Shared components first (if any)
      #     kubectl -n "$sys_ns" get deploy -o name | xargs -r -L1 kubectl -n "$sys_ns" rollout status --timeout=10m || true
      #     kubectl -n "$sys_ns" get sts   -o name | xargs -r -L1 kubectl -n "$sys_ns" rollout status --timeout=10m || true

      #     # 3) Wait until some workload shows in app namespace
      #     end=$((SECONDS+600))
      #     until [ "$(kubectl -n "$app_ns" get pods --no-headers 2>/dev/null | wc -l | xargs || echo 0)" -gt 0 ]; do
      #       [ $SECONDS -ge $end ] && {
      #         echo "timeout: no pods appeared in $app_ns"
      #         echo "::group::All resources across namespaces"
      #         kubectl get all -A
      #         echo "::endgroup::"
      #         echo "::group::Recent events"
      #         kubectl get events -A --sort-by=.lastTimestamp | tail -n 200 || true
      #         echo "::endgroup::"
      #         exit 1
      #       }
      #       sleep 5
      #     done

      #     # 4) Rollout controllers in the app namespace (if present), then wait for pods
      #     kubectl -n "$app_ns" get deploy -o name | xargs -r -L1 kubectl -n "$app_ns" rollout status --timeout=10m || true
      #     kubectl -n "$app_ns" get sts   -o name | xargs -r -L1 kubectl -n "$app_ns" rollout status --timeout=10m || true

      #     pods_exist=$(kubectl -n "$app_ns" get pod -o name | wc -l | xargs || echo 0)
      #     if [ "$pods_exist" -gt 0 ]; then
      #       kubectl -n "$app_ns" wait pod -l '!job-name' --for=condition=Ready --timeout=10m || true
      #     fi

      #     kubectl -n "$app_ns" get all -o wide

      # - name: Show services and endpoints
      #   run: |
      #     echo "Listing services in namespace ${SOLO_NAMESPACE}"
      #     kubectl get svc -n ${SOLO_NAMESPACE}

      # - name: Port forward (if services exist)
      #   run: |
      #     set -euxo pipefail
      #     if kubectl get svc haproxy-node1-svc -n ${SOLO_NAMESPACE} >/dev/null 2>&1; then
      #       kubectl port-forward svc/haproxy-node1-svc -n ${SOLO_NAMESPACE} 50211:50211 &
      #     else
      #       echo "HAProxy service haproxy-node1-svc not found, skipping port-forward"
      #     fi

      #     if kubectl get svc envoy-proxy-node1-svc -n ${SOLO_NAMESPACE} >/dev/null 2>&1; then
      #       kubectl port-forward svc/envoy-proxy-node1-svc -n ${SOLO_NAMESPACE} 8080:8080 &
      #     else
      #       echo "gRPC proxy service envoy-proxy-node1-svc not found, skipping port-forward"
      #     fi

      #     if kubectl get svc mirror-1-rest -n ${SOLO_NAMESPACE} >/dev/null 2>&1; then
      #       kubectl port-forward svc/mirror-1-rest -n ${SOLO_NAMESPACE} 5551:80 &
      #     else
      #       echo "Mirror node service mirror-1-rest not found, skipping port-forward"
      #     fi

      #     if kubectl get svc mirror-1-grpc -n ${SOLO_NAMESPACE} >/dev/null 2>&1; then
      #       kubectl port-forward svc/mirror-1-grpc -n ${SOLO_NAMESPACE} 5600:5600 &
      #     else
      #       echo "Mirror node service mirror-1-grpc not found, skipping port-forward"
      #     fi

      #     if kubectl get svc mirror-1-web3 -n ${SOLO_NAMESPACE} >/dev/null 2>&1; then
      #       kubectl port-forward svc/mirror-1-web3 -n ${SOLO_NAMESPACE} 8545:80 &
      #     else
      #       echo "Mirror node service mirror-1-web3 not found, skipping port-forward"
      #     fi

      #     if kubectl get svc mirror-1-restjava -n ${SOLO_NAMESPACE} >/dev/null 2>&1; then
      #       kubectl port-forward svc/mirror-1-restjava -n ${SOLO_NAMESPACE} 8084:80 &
      #     else
      #       echo "Java REST API service mirror-1-restjava not found, skipping port-forward"
      #     fi

      # # - name: Create ECDSA account
      # #   run: |
      # #     npm run solo -- ledger account create --generate-ecdsa-key --deployment ${SOLO_DEPLOYMENT}
      # #     npm run solo -- ledger account update --account-id 0.0.1002 --hbar-amount 1000000 --deployment ${SOLO_DEPLOYMENT}

      # - name: Write .env for tests
      #   run: |
      #     umask 077
      #     {
      #       echo "TEST_NETWORK_NAME=localhost"
      #       echo "TEST_RUN_NONFREE=1"
      #       echo "TEST_OPERATOR_ID=0.0.2"
      #       echo "TEST_OPERATOR_KEY=302e020100300506032b65700422042091132178e72057a1d7528025956fe39b0b847f200ab59b2fdd367017f3087137"
      #     } > .env
      #     cat .env

      # TODO: Uncomment when Solo network is set up and ready to run integration tests
      # - name: Test
      #   run: swift test

      # - name: Cleanup KinD (always)
      #   if: always()
      #   run: |
      #     kind delete cluster --name "${SOLO_CLUSTER_NAME}" || true
