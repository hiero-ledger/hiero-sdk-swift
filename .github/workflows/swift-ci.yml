name: Swift CI
on:
  pull_request:
  push: 
    branches: ['main']

defaults:
  run:
    shell: bash

permissions:
  contents: read

jobs:
  format:
    runs-on: macos-latest
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@0080882f6c36860b6ba35c610c98ce87d4e2f26f # v2.10.2
        with:
          egress-policy: audit

      - name: Checkout Code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

      - name: Install swift-format
        run: brew install swift-format

      - name: Format
        run: swift format lint --strict --configuration .swift-format.json --recursive --parallel Sources/ Tests/ Examples/ Package.swift

  # build:
  #   strategy:
  #     matrix:
  #       swift: ["5.9"]
  #       os: [macos-13]

  #   runs-on: ${{ matrix.os }}
  #   steps:
  #     - name: Harden Runner
  #       uses: step-security/harden-runner@0080882f6c36860b6ba35c610c98ce87d4e2f26f # v2.10.2
  #       with:
  #         egress-policy: audit

  #     - name: Setup Swift
  #       uses: swift-actions/setup-swift@3aed395c5397f62deb91d8fe7af1418a9ae4d16f # v2.1.0
  #       with:
  #         swift-version: ${{ matrix.swift }}

  #     - name: Checkout Code
  #       uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

  #     - name: Cache
  #       uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0
  #       with:
  #         path: sdk/swift/.build
  #         key: ${{ runner.os }}-${{ matrix.swift }}-spm-${{ github.job }}-${{ hashFiles('**/Package.resolved') }}
  #         restore-keys: |
  #           ${{ runner.os }}-${{ matrix.swift }}-spm-
  #     - name: Build
  #       run: swift build

  test:
    strategy:
      matrix:
        swift: ["5.9"]
        os: [macos-13]
    runs-on: ${{ matrix.os }}
    env:
      SOLO_CLUSTER_NAME: solo
      SOLO_NAMESPACE: solo
      SOLO_CLUSTER_SETUP_NAMESPACE: solo-cluster
      SOLO_DEPLOYMENT: solo-deployment
      KIND_IMAGE: kindest/node:v1.32.2   # pin to a stable image

    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@0080882f6c36860b6ba35c610c98ce87d4e2f26f
        with:
          egress-policy: audit

      - name: Setup Swift
        uses: swift-actions/setup-swift@3aed395c5397f62deb91d8fe7af1418a9ae4d16f
        with:
          swift-version: ${{ matrix.swift }}

      - name: Checkout Code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683

      - name: Cache Code
        uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57
        with:
          path: sdk/swift/.build
          key: ${{ runner.os }}-${{ matrix.swift }}-spm-${{ github.job }}-${{ hashFiles('**/Package.resolved') }}
          restore-keys: |
            ${{ runner.os }}-${{ matrix.swift }}-spm-

      # ---------- Tooling ----------
      - name: Install kind / kubectl / Helm / utils
        run: |
          set -euxo pipefail
          brew update
          brew install kind kubectl helm jq coreutils

      - name: Remove Preinstalled Docker (clean path)
        run: |
          set -x
          sudo killall dockerd || true
          sudo killall containerd || true
          sudo rm -rvf /usr/bin/*containerd* || true
          sudo rm -rvf /usr/bin/docker* || true
          sudo rm -rvf /usr/local/bin/docker* || true
          sudo rm -rvf /usr/local/bin/*lima* || true

      - name: Start Colima (Docker runtime, more resources)
        run: |
          set -euxo pipefail
          brew install docker colima
          colima start --vm-type=vz --runtime docker --cpu=4 --memory=8 --disk=20
          docker context use colima
          docker version
          docker info | sed -n '1,120p'

      - name: Show Colima Logs (on failure)
        if: failure()
        run: |
          echo "::group::Serial Logs"
          cat ${HOME}/.colima/_lima/colima/serial*.log || true
          echo "::endgroup::"
          echo "::group::Error Logs"
          cat ${HOME}/.colima/_lima/colima/ha.stderr.log || true
          echo "::endgroup::"

      # ---------- Node.js for Solo ----------
      - name: Use Node 20.18.x
        uses: actions/setup-node@v4
        with:
          node-version: "20.18.x"

      - name: Install Solo CLI (v0.38.x)
        run: |
          npm install -g @hashgraph/solo@0.38
          solo --version

      # ---------- Create a local K8s cluster with KinD (robust) ----------
      - name: Pre-pull KinD node image
        run: docker pull "${KIND_IMAGE}"

      - name: Create KinD cluster (retry + wait)
        run: |
          set -euxo pipefail
          retry() { n=0; until [ $n -ge 3 ]; do "$@" && break; n=$((n+1)); echo "retry $n..."; sleep $((5*n)); done; [ $n -lt 3 ]; }
          # Clean any leftovers
          kind get clusters | xargs -r -I{} kind delete cluster --name {}
          retry kind create cluster --name "${SOLO_CLUSTER_NAME}" --image "${KIND_IMAGE}" --wait 5m
          kubectl cluster-info --context "kind-${SOLO_CLUSTER_NAME}"
          kubectl get nodes -o wide

      - name: Debug kube-apiserver (on failure)
        if: failure()
        run: |
          set -x
          NODE="${SOLO_CLUSTER_NAME}-control-plane"
          docker ps -a || true
          docker logs "${NODE}" | sed -n '1,200p' || true
          docker exec -i "${NODE}" crictl --runtime-endpoint /run/containerd/containerd.sock ps -a | grep kube-apiserver || true
          APISERVER_ID="$(docker exec -i "${NODE}" crictl --runtime-endpoint /run/containerd/containerd.sock ps -a | awk '/kube-apiserver/ {print $1; exit}')"
          if [ -n "${APISERVER_ID}" ]; then
            docker exec -i "${NODE}" crictl --runtime-endpoint /run/containerd/containerd.sock logs "${APISERVER_ID}" || true
          fi
          kind export logs --name "${SOLO_CLUSTER_NAME}" ./kind-logs || true

      - name: Upload kind logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: kind-logs
          path: ./kind-logs

      # ---------- Solo initialization & deployment ----------
      - name: Solo init
        run: |
          set -euxo pipefail
          # Do NOT delete/recreate the cluster here anymore
          rm -rf ~/.solo || true
          solo init

      - name: Connect cluster-ref and create deployment
        run: |
          solo cluster-ref connect --cluster-ref "kind-${SOLO_CLUSTER_NAME}" --context "kind-${SOLO_CLUSTER_NAME}"
          solo deployment create -n "${SOLO_NAMESPACE}" --deployment "${SOLO_DEPLOYMENT}"

      - name: Add a cluster (1 consensus node)
        run: |
          solo deployment add-cluster \
            --deployment "${SOLO_DEPLOYMENT}" \
            --cluster-ref "kind-${SOLO_CLUSTER_NAME}" \
            --num-consensus-nodes 1

      - name: Generate node keys (gossip + TLS)
        run: |
          solo node keys --gossip-keys --tls-keys --deployment "${SOLO_DEPLOYMENT}"
          ls -la ~/.solo/cache/keys || true

      - name: Setup cluster shared components
        run: solo cluster-ref setup -s "${SOLO_CLUSTER_SETUP_NAMESPACE}"

      - name: Deploy network (consensus node only)
        run: |
          set -x
          solo network deploy --deployment "${SOLO_DEPLOYMENT}" --cluster-ref "kind-${SOLO_CLUSTER_NAME}"

      - name: Set up Hedera node
        run: solo node setup --deployment "${SOLO_DEPLOYMENT}" --cluster-ref "kind-${SOLO_CLUSTER_NAME}"

      - name: Start Hedera node(s)
        run: solo node start --deployment "${SOLO_DEPLOYMENT}" --cluster-ref "kind-${SOLO_CLUSTER_NAME}"

      - name: Wait for workloads to be Ready (robust)
        run: |
          set -euxo pipefail
          ns="${SOLO_NAMESPACE}"

          # 1) Wait for the namespace to exist (created by Solo)
          end=$((SECONDS+300))
          until kubectl get ns "$ns" >/dev/null 2>&1; do
            [ $SECONDS -ge $end ] && { echo "timeout: namespace $ns not found"; kubectl get ns; exit 1; }
            sleep 3
          done

          # 2) Wait until at least one pod shows up in the namespace
          end=$((SECONDS+600))
          until [ "$(kubectl -n "$ns" get pods --no-headers 2>/dev/null | wc -l | xargs || echo 0)" -gt 0 ]; do
            [ $SECONDS -ge $end ] && { echo "timeout: no pods appeared in $ns"; kubectl -n "$ns" get all; exit 1; }
            sleep 5
          done

          # 3) Wait for controllers to roll out (only if they exist)
          kubectl -n "$ns" get deploy -o name | xargs -r -L1 kubectl -n "$ns" rollout status --timeout=10m
          kubectl -n "$ns" get sts   -o name | xargs -r -L1 kubectl -n "$ns" rollout status --timeout=10m

          # 4) Wait for long-running pods to be Ready (exclude short-lived job pods)
          pods_exist=$(kubectl -n "$ns" get pod -o name | wc -l | xargs || echo 0)
          if [ "$pods_exist" -gt 0 ]; then
            kubectl -n "$ns" wait pod -l '!job-name' --for=condition=Ready --timeout=10m || true
          fi

          kubectl -n "$ns" get all -o wide


      - name: Show services and endpoints
        if: always()
        run: |
          echo "Solo services in ${SOLO_NAMESPACE}:"
          kubectl -n "${SOLO_NAMESPACE}" get svc
          echo "Solo endpoints (if any):"
          kubectl -n "${SOLO_NAMESPACE}" get endpoints

      - name: Write .env for tests
        run: |
          umask 077
          {
            echo "TEST_NETWORK_NAME=localhost"
            echo "TEST_RUN_NONFREE=1"
            echo "TEST_OPERATOR_ID=0.0.2"
            echo "TEST_OPERATOR_KEY=302e020100300506032b65700422042091132178e72057a1d7528025956fe39b0b847f200ab59b2fdd367017f3087137"
          } > .env
          cat .env

      - name: Test
        run: swift test

      - name: Cleanup KinD / Colima (always)
        if: always()
        run: |
          kind delete cluster --name "${SOLO_CLUSTER_NAME}" || true
          colima stop || true
          LOGDIR="$HOME/.colima/_lima/colima"
          echo "=== ha.stderr.log ===" ; cat "$LOGDIR/ha.stderr.log" || true
          echo "=== serial logs ==="   ; cat "$LOGDIR"/serial*.log   || true
